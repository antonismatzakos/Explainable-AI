# Explainable-AI Lime

LIME
LIME is model-agnostic, which means it may
be used with any machine learning model. The
technique tries to figure out what the model is doing by perturbating input images and seeing how 
the predictions change. LIME generates explanations that indicate the contribution of each characteristic to the data sample prediction. Generally
speaking, three steps are necessary to get visual
explanations with LIME. First, it is necessary to
generate a dataset of perturbed images. For image
datasets this means random areas of an image
are "greyed-out". Second, we obtain predictions for
those images with the previously trained neural
network. Third, a locally linear model is learned
to identify mistakes made in perturbed images. As
a last step the visual explanation is generated by
presenting the most important pixels with the highest weights and omitting other parts of the image
[33]. The final result can be inspected in Figure
8 as the original input image and Figure 9 as the
XAI-output of the LIME method.
